# Task 4: Lexical Analysis and Parsing

## 📋 Task Overview

This task marked my transition from understanding existing compilation tools to building my own compiler components. Instead of just using GCC, I learned to create the fundamental building blocks of any compiler: the **lexical analyzer** (tokenizer) and **parser**.

**The Challenge:** How do we teach a computer to understand human-readable text and verify it follows language rules?

**My Solution:** Build a simple language processor that recognizes English sentence structure using industry-standard compiler tools.

**Why this matters:** Every programming language needs to break down source code into tokens and verify syntax. This task teaches the foundation of how all compilers work.

## 📂 File Structure and Generated Files

```
T4_Parsing/
├── HelloWorld.c        # Simple C program (for comparison with previous tasks)
├── prog1.l            # Flex lexical analyzer specification (my rules for tokens)
├── prog1.y            # Bison parser grammar specification (my grammar rules)
├── input.txt          # Test input for the parser ("I eat rice")
├── output.txt         # Generated output from parsing
├── lex.yy.c          # Generated C code for lexical analyzer (created by Flex)
├── prog1.tab.c       # Generated C code for parser (created by Bison)
├── prog1.tab.h       # Generated header file with token definitions
├── parser.exe        # Final executable parser program
├── Makefile          # Build automation for the complete process
└── README.md         # This documentation
```

## 🧠 The Language I'm Parsing

**Grammar:** Simple Subject-Verb-Object sentences
**Valid sentences:** "I eat rice", "i EAT Rice", etc.
**Invalid sentences:** "You eat rice", "I rice", "eat rice"

**My Language Specification:**
- **Subject:** "I" or "i" (case insensitive)
- **Verb:** "eat", "Eat", "EAT" (case insensitive)
- **Object:** "rice", "Rice" (case insensitive)
- **Structure:** Must be in exact order: Subject → Verb → Object

## 📝 Building the Lexical Analyzer

### `prog1.l` - Teaching the Computer to Recognize Words

```lex
%{
#include "prog1.tab.h"
%}

%%
[Ii]           { printf("Noun\n"); return N; }
[eE]at         { printf("Verb\n"); return V; }
[rR]ice        { printf("Object\n"); return O; }
[ \t\n]+       { /* skip whitespace */ }
.              { printf("Unknown Token: %s\n", yytext); return -1; }
%%

int yywrap() {
    return 1;
}
```

### Understanding the Flex Structure

#### Header Section
```c
%{
#include "prog1.tab.h"
%}
```
**What this does:**
- Includes the parser header file (generated by Bison)
- Provides access to token constants (N, V, O)
- Links lexer and parser together

**Why necessary:** The lexer needs to know what token types the parser expects

#### Rules Section - The Heart of Tokenization

**Rule 1: Recognizing Subjects**
```lex
[Ii]           { printf("Noun\n"); return N; }
```
**Breaking it down:**
- **Pattern:** `[Ii]` is a regular expression matching 'I' or 'i'
- **Action:** Code executed when pattern matches
- **printf("Noun\n"):** Debugging output (shows what was recognized)
- **return N:** Sends token type N to parser
- **Character class:** `[Ii]` means "any one character from this set"

**Rule 2: Recognizing Verbs**
```lex
[eE]at         { printf("Verb\n"); return V; }
```
**Pattern breakdown:**
- **[eE]:** First character is 'e' or 'E'
- **at:** Followed by literal characters 'a' and 't'
- **Result:** Matches "eat", "Eat", "EAT", "eAt", etc.

**Rule 3: Recognizing Objects**
```lex
[rR]ice        { printf("Object\n"); return O; }
```
**Same principle:** Case-insensitive matching for "rice"

**Rule 4: Handling Whitespace**
```lex
[ \t\n]+       { /* skip whitespace */ }
```
**Pattern explanation:**
- **[ \t\n]:** Character class with space, tab, and newline
- **+:** One or more occurrences
- **Action:** Empty (ignore these characters)
- **Purpose:** Allows "I eat rice" instead of requiring "Ieatrice"

**Rule 5: Error Handling**
```lex
.              { printf("Unknown Token: %s\n", yytext); return -1; }
```
**Catch-all rule:**
- **.:** Matches any single character not matched by previous rules
- **yytext:** Built-in variable containing the matched text
- **return -1:** Error signal to parser

#### Function Section
```c
int yywrap() {
    return 1;
}
```
**Required by Flex:**
- Called when input is exhausted
- **return 1:** No more input files to process
- **return 0:** Would indicate more files to process

## 📝 Building the Parser

### `prog1.y` - Teaching Grammar Rules

```bison
%{
#include <stdio.h>
void yyerror(const char *s);
int yylex();
%}

%token N V O
%start S

%%
S: N V O { printf("Parsing Finished\n"); }
;
%%

void yyerror(const char *s) {
    fprintf(stderr, "Error: %s\n", s);
}

int main() {
    return yyparse();
}
```

### Understanding the Bison Structure

#### Header Section
```c
%{
#include <stdio.h>
void yyerror(const char *s);
int yylex();
%}
```
**Function declarations:**
- **yyerror:** Called when syntax errors occur
- **yylex:** Calls the lexical analyzer to get next token

#### Token and Grammar Declarations
```bison
%token N V O
%start S
```
**Token definitions:**
- **%token N V O:** Declares terminal symbols (tokens from lexer)
- **%start S:** Declares starting symbol of grammar

**These correspond to:**
- **N:** Noun token (subject)
- **V:** Verb token (action)
- **O:** Object token (target)
- **S:** Sentence (complete valid input)

#### Grammar Rules Section
```bison
S: N V O { printf("Parsing Finished\n"); }
;
```

**Grammar rule breakdown:**
- **S:** Left-hand side (what we're defining)
- **N V O:** Right-hand side (sequence that makes a valid S)
- **{...}:** Semantic action (code executed when rule matches)
- **;:** Ends the rule

**In plain English:** "A sentence (S) consists of a noun (N) followed by a verb (V) followed by an object (O)"

#### Function Implementations

**Error handling:**
```c
void yyerror(const char *s) {
    fprintf(stderr, "Error: %s\n", s);
}
```
**Called when:** Parser encounters unexpected token or end of input
**Purpose:** Provide meaningful error messages

**Main function:**
```c
int main() {
    return yyparse();
}
```
**Program entry point:**
- **yyparse():** Generated parser function
- **Returns:** 0 on success, 1 on syntax error

## 🔧 Build Process Deep Dive

The Makefile orchestrates the complete build process:

```makefile
main1:
	gcc -o Hello HelloWorld.c

main2:
	bison -d prog1.y
	flex prog1.l
	gcc lex.yy.c prog1.tab.c -o parser.exe
	parser.exe < input.txt > output.txt

main3:
	bison -d prog1.y
	flex prog1.l
	gcc lex.yy.c prog1.tab.c -o parser.exe
	parser
```

### Target 1: Simple Comparison (`main1`)
```makefile
gcc -o Hello HelloWorld.c
```
**Purpose:** Shows difference between regular C compilation and parser generation

### Target 2: Parser with File I/O (`main2`)

#### Step 1: Generate Parser Code
```makefile
bison -d prog1.y
```
**What bison does:**
- **Input:** `prog1.y` (grammar specification)
- **Output:** `prog1.tab.c` (parser implementation) and `prog1.tab.h` (token definitions)
- **-d flag:** Generate header file with token definitions

**Generated files contain:**
- **Parsing tables:** State machine for syntax analysis
- **Token constants:** #define N 258, #define V 259, etc.
- **yyparse() function:** Main parsing logic

#### Step 2: Generate Lexer Code
```makefile
flex prog1.l
```
**What flex does:**
- **Input:** `prog1.l` (lexical specification)
- **Output:** `lex.yy.c` (lexical analyzer implementation)

**Generated code contains:**
- **yylex() function:** Reads input and returns tokens
- **State machine:** Efficiently matches regular expressions
- **Token recognition:** Converts text to token numbers

#### Step 3: Compile Everything Together
```makefile
gcc lex.yy.c prog1.tab.c -o parser.exe
```
**Links together:**
- **lex.yy.c:** Lexical analyzer (tokenizer)
- **prog1.tab.c:** Parser (syntax analyzer)
- **Result:** Complete language processor

#### Step 4: Run with Input/Output Redirection
```makefile
parser.exe < input.txt > output.txt
```
**I/O redirection:**
- **< input.txt:** Feed file contents as standard input
- **> output.txt:** Capture output to file
- **Automated testing:** Run parser without manual input

### Target 3: Interactive Mode (`main3`)
Same build process, but runs parser interactively for experimentation

## 🔄 The Complete Parsing Flow

```
input.txt ("I eat rice")
    │
    ▼
┌─────────────┐    ┌──────────────┐    ┌─────────────┐
│ Lexer       │───→│ Parser       │───→│ Success/    │
│ (lex.yy.c)  │    │ (prog1.tab.c)│    │ Error       │
└─────────────┘    └──────────────┘    └─────────────┘
    │                      │
    ▼                      ▼
"I" → N token          N V O → Valid sentence
"eat" → V token        
"rice" → O token       
```

### Detailed Token Flow:

1. **Input:** "I eat rice"
2. **Lexer reads 'I':** Matches `[Ii]` → returns N
3. **Lexer reads 'eat':** Matches `[eE]at` → returns V
4. **Lexer reads 'rice':** Matches `[rR]ice` → returns O
5. **Parser receives:** N V O sequence
6. **Parser matches:** S → N V O rule
7. **Result:** "Parsing Finished"

### Error Example:

1. **Input:** "You eat rice"
2. **Lexer reads 'You':** Matches `.` → returns -1 (error)
3. **Parser receives:** Error token
4. **Result:** "Error: syntax error"

## 🚀 How to Build and Test

### Build and run with file input:
```bash
make main2
cat output.txt
```

### Build and run interactively:
```bash
make main3
# Type: I eat rice
# Press Ctrl+D (Windows) or Ctrl+Z (Linux)
```

### Test different inputs:
```bash
echo "I eat rice" | ./parser.exe
echo "i EAT Rice" | ./parser.exe
echo "You eat rice" | ./parser.exe    # Should fail
echo "I rice" | ./parser.exe          # Should fail
```

## 📋 Expected Outputs

### Valid input ("I eat rice"):
```
Noun
Verb
Object
Parsing Finished
```

### Invalid input ("You eat rice"):
```
Unknown Token: Y
Error: syntax error
```

### Invalid input ("I rice"):
```
Noun
Object
Error: syntax error
```

## 🎯 Understanding Token vs Parse Errors

### Lexical Errors (Unknown Tokens)
**Input:** "You eat rice"
**Problem:** "You" doesn't match any token pattern
**Error location:** Lexer
**Message:** "Unknown Token: Y"

### Syntax Errors (Wrong Grammar)
**Input:** "I rice"
**Problem:** Missing verb, wrong sentence structure
**Error location:** Parser
**Message:** "Error: syntax error"

**Learning:** Lexical analysis (tokenization) happens before syntax analysis (parsing)

## 🔍 Deep Dive: Generated Code Analysis

### Examining `lex.yy.c`:
```bash
grep -n "case [0-9]:" lex.yy.c
```
Shows the state machine that recognizes patterns

### Examining `prog1.tab.c`:
```bash
grep -n "case [0-9]:" prog1.tab.c
```
Shows the parsing state machine

### Examining `prog1.tab.h`:
```bash
cat prog1.tab.h
```
Shows token constant definitions

## 🎯 Key Learning Outcomes

### Lexical Analysis Concepts
- **Regular expressions:** Pattern matching for tokens
- **Finite automata:** How lexers recognize patterns efficiently
- **Token classification:** Converting character sequences to meaningful units
- **Error handling:** Dealing with unrecognized input

### Syntax Analysis Concepts
- **Context-free grammars:** Rules for valid language structure
- **Parse trees:** How syntax is represented
- **Bottom-up parsing:** LR parsing technique used by Bison
- **Error recovery:** Handling syntax errors gracefully

### Tool Understanding
- **Flex:** Industrial-strength lexer generator
- **Bison:** Production-quality parser generator
- **Integration:** How lexer and parser communicate
- **Code generation:** How specifications become executable code

### Compiler Architecture
- **Frontend design:** Lexical → Syntactic → Semantic analysis
- **Tool-based development:** Using generators vs hand-coding
- **Language specification:** Formal definition of syntax
- **Testing strategies:** How to verify language processors

## 🔧 Experiments to Deepen Understanding

### 1. Extend the vocabulary:
Add "apple" as another object:
```lex
[rR]ice|[aA]pple    { printf("Object\n"); return O; }
```

### 2. Add more sentence patterns:
```bison
S: N V O         { printf("Simple sentence\n"); }
 | N V O V O     { printf("Compound sentence\n"); }
;
```

### 3. Add more tokens:
```lex
[hH]e|[sS]he      { printf("Pronoun\n"); return N; }
[dD]rink          { printf("Verb\n"); return V; }
[wW]ater          { printf("Object\n"); return O; }
```

### 4. Add semantic actions:
```bison
S: N V O { printf("Subject: %s, Action: %s, Object: %s\n", $1, $2, $3); }
```

## 💡 Real-World Applications

### Programming Language Compilers
- **C compiler:** Tokenizes keywords, identifiers, operators
- **Python interpreter:** Handles indentation-sensitive syntax
- **JavaScript engines:** Parses complex expression syntax

### Domain-Specific Languages
- **SQL:** Database query parsing
- **HTML/XML:** Markup language processing
- **Configuration files:** .ini, .yaml, .json parsers
- **Mathematical expressions:** Calculator applications

### Text Processing Tools
- **grep:** Regular expression matching
- **sed/awk:** Pattern-based text transformation
- **Syntax highlighters:** Code editors and IDEs
- **Documentation processors:** Markdown, LaTeX

## 🔮 Preparing for Advanced Compiler Topics

This foundation prepares you for:

### Semantic Analysis
- **Symbol tables:** Tracking variable declarations and types
- **Type checking:** Ensuring operations are valid
- **Scope analysis:** Understanding variable visibility

### Intermediate Code Generation
- **Abstract syntax trees:** Internal representation of parsed code
- **Three-address code:** Simplified instruction format
- **Control flow graphs:** Representing program structure

### Optimization
- **Dead code elimination:** Removing unused code
- **Constant folding:** Compile-time computation
- **Loop optimization:** Improving repetitive code

### Code Generation
- **Target machine instructions:** Converting to assembly
- **Register allocation:** Efficient use of CPU registers
- **Instruction selection:** Choosing optimal instruction sequences

## 🌟 Industry Perspective

### Why These Tools Matter
- **Flex and Bison:** Used in production compilers (GCC, LLVM tools)
- **Proven technology:** Decades of industrial use
- **Standards compliance:** POSIX lex and yacc compatibility
- **Performance:** Highly optimized generated code

### Career Applications
- **Compiler development:** Working on language implementations
- **Tool development:** Building developer productivity tools
- **System software:** Operating system components
- **Domain expertise:** Understanding how software tools work internally

Understanding lexical analysis and parsing is fundamental to:
- Building interpreters and compilers
- Creating domain-specific languages
- Developing sophisticated text processing tools
- Understanding how programming languages work

This task represents the transition from using compilers to building them!
